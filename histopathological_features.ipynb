{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ddd205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from lifelines.utils import concordance_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee048d7",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd613243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TileDataset(Dataset):\n",
    "    def __init__(self, img_dir, tissue, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.tissue = tissue\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        tissue_labels = pd.read_csv('./tissue_labels.csv', index_col = 'tile')\n",
    "        condition = tissue_labels['tissue'] == tissue\n",
    "        tissue_labels_tissue_index = list(tissue_labels[condition].index)\n",
    "        \n",
    "        labelcsv = pd.read_csv('./survival_COAD_survival.csv', index_col = 'sample')\n",
    "        \n",
    "        imagelist = []\n",
    "        labellist = []\n",
    "        \n",
    "        for file in tissue_labels_tissue_index:\n",
    "            imagelist.append(file)\n",
    "            \n",
    "            OS = labelcsv.at[file[0:15], 'OS']\n",
    "            OS_time = labelcsv.at[file[0:15], 'OS.time']\n",
    "            labellist.append(torch.tensor([OS, OS_time]))\n",
    "        \n",
    "        self.imagelist = imagelist\n",
    "        self.labellist = labellist\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imagelist)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.imagelist[idx][0:23], self.imagelist[idx])\n",
    "        image = Image.open(img_path)\n",
    "        label = self.labellist[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bc3e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "ADI_Tile  = TileDataset('./randomsampling', 0, transform = transform)\n",
    "BACK_Tile = TileDataset('./randomsampling', 1, transform = transform)\n",
    "DEB_Tile  = TileDataset('./randomsampling', 2, transform = transform)\n",
    "LYM_Tile  = TileDataset('./randomsampling', 3, transform = transform)\n",
    "MUC_Tile  = TileDataset('./randomsampling', 4, transform = transform)\n",
    "MUS_Tile  = TileDataset('./randomsampling', 5, transform = transform)\n",
    "NORM_Tile = TileDataset('./randomsampling', 6, transform = transform)\n",
    "STR_Tile  = TileDataset('./randomsampling', 7, transform = transform)\n",
    "TUM_Tile  = TileDataset('./randomsampling', 8, transform = transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84ac273",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357833c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60% training, 20% validation, and 20% testing\n",
    "\n",
    "def split_dataset(dataset, BS = 64):\n",
    "    original_targets = [i[0] for i in dataset.labellist]\n",
    "    train_val_index, test_index = train_test_split(np.arange(len(dataset)), test_size=0.2, random_state=0, stratify=original_targets)\n",
    "    \n",
    "    train_val_targets = []\n",
    "    sort_train_val_index = sorted(train_val_index)\n",
    "    for i in sort_train_val_index:\n",
    "        train_val_targets.append(original_targets[i])\n",
    "    \n",
    "    train_index, val_index = train_test_split(sort_train_val_index, test_size=0.25, random_state=0, stratify=train_val_targets)\n",
    "    \n",
    "    train_dataset = Subset(dataset, train_index)\n",
    "    val_dataset = Subset(dataset, val_index)\n",
    "    test_dataset = Subset(dataset, test_index)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BS, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=BS, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    dataloader = [train_dataloader, val_dataloader, test_dataloader]\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03bd638",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8734e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepConvSurv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepConvSurv, self).__init__()\n",
    "        # 3*224*224\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 7, stride = 3),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        # 32*36*36\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 5, stride = 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # 32*16*16\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        # 32*3*3\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(32 * 3 * 3, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        # 32\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        x = x.view(-1, 3 * 3 * 32)\n",
    "        x = self.fc1(x)\n",
    "        self.feature = x.detach()\n",
    "        \n",
    "        risks = self.fc2(x)\n",
    "        \n",
    "        return risks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194a331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.0)\n",
    "    elif type(m) == nn.Linear:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.0)\n",
    "    elif type(m) == nn.BatchNorm2d:\n",
    "        nn.init.constant_(m.weight, 1.0)\n",
    "        nn.init.constant_(m.bias, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eac8847",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepConvSurv()\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b2d612",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5966d001",
   "metadata": {},
   "outputs": [],
   "source": [
    "class negative_log_partial_likelihood(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(negative_log_partial_likelihood, self).__init__()\n",
    "    \n",
    "    def forward(self, risk, os, os_time, model = None, regularization = False, Lambda = 1e-05):\n",
    "        # R_matrix\n",
    "        batch_len = risk.shape[0]\n",
    "        R_matrix = np.zeros([batch_len, batch_len], dtype=int)\n",
    "        for i in range(batch_len):\n",
    "            for j in range(batch_len):\n",
    "                R_matrix[i,j] = os_time[j] >= os_time[i]\n",
    "        R_matrix = torch.tensor(R_matrix, dtype = torch.float32)\n",
    "        R_matrix = R_matrix.to('cuda')\n",
    "        \n",
    "        # exp_theta\n",
    "        theta = risk\n",
    "        exp_theta = torch.exp(theta)\n",
    "    \n",
    "        # negative_log_partial_likelihood\n",
    "        loss = - torch.sum( (theta - torch.log(torch.sum( exp_theta*R_matrix ,dim=1)) ) * os.float() ) / torch.sum(os)\n",
    "    \n",
    "        # l1 regularization\n",
    "        l1_reg = torch.zeros(1)\n",
    "        if regularization == True:\n",
    "            for param in model.parameters():\n",
    "                l1_reg = l1_reg + torch.sum(torch.abs(param))\n",
    "            return loss + Lambda * l1_reg\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb39a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = negative_log_partial_likelihood()\n",
    "#print(loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b80223",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eded1b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_update = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "            \n",
    "optimizer = optim.Adam(params_to_update, lr=1e-04)\n",
    "#print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53de3ab",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e27cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, num_epochs = 10):\n",
    "    \n",
    "    since = time.time()\n",
    "    \n",
    "    c_index_history = []\n",
    "    loss_history = []\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_c_index = 0.0\n",
    "    \n",
    "    model = model.to('cuda')\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        model = model.to('cuda')\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                Dataloader = dataloader[0]\n",
    "            else:\n",
    "                model.eval()\n",
    "                Dataloader = dataloader[1]   \n",
    "                \n",
    "            risk_all = None\n",
    "            label_all = None\n",
    "            running_loss = 0.0\n",
    "            iteration = 0\n",
    "        \n",
    "            for inputs, labels in tqdm(Dataloader):\n",
    "                iteration = iteration + 1\n",
    "            \n",
    "                inputs = inputs.to('cuda')\n",
    "                labels = labels.to('cuda')\n",
    "            \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "            \n",
    "                    if iteration == 1:\n",
    "                        risk_all = outputs\n",
    "                        label_all = labels\n",
    "                    else:\n",
    "                        risk_all = torch.cat([risk_all, outputs])\n",
    "                        label_all = torch.cat([label_all, labels])\n",
    "            \n",
    "                    # loss\n",
    "                    loss = loss_fn(risk = outputs, os = labels[:,0], os_time = labels[:,1])\n",
    "            \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                # statistics\n",
    "                running_loss = running_loss + loss.item() * torch.sum(labels[:,0]).item() \n",
    "            \n",
    "            epoch_loss = running_loss / torch.sum(label_all[:,0]).item()\n",
    "        \n",
    "            OS_time = label_all[:,1].detach().cpu().numpy()\n",
    "            HR = risk_all.detach().cpu().numpy()\n",
    "            OS = label_all[:,0].detach().cpu().numpy()\n",
    "        \n",
    "            epoch_c_index = concordance_index(OS_time, -HR.reshape(-1), OS)\n",
    "            print('{} Loss: {:.4f} C-index: {:.4f}'.format(phase, epoch_loss, epoch_c_index))\n",
    "        \n",
    "            if phase == 'val' and epoch_c_index > best_c_index:\n",
    "                best_c_index = epoch_c_index\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                c_index_history.append(epoch_c_index)\n",
    "                loss_history.append(epoch_loss)\n",
    "                \n",
    "            time.sleep(1)\n",
    "            \n",
    "        print('-' * 40)\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best Val C-index: {:f}'.format(best_c_index))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    print('-' * 40)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # test\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    model.eval()\n",
    "    \n",
    "    risk_all = None\n",
    "    label_all = None\n",
    "    iteration = 0\n",
    "    for inputs, labels in tqdm(dataloader[2]):\n",
    "        iteration = iteration + 1\n",
    "        \n",
    "        inputs = inputs.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "            if iteration == 1:\n",
    "                risk_all = outputs\n",
    "                label_all = labels\n",
    "            else:\n",
    "                risk_all = torch.cat([risk_all, outputs])\n",
    "                label_all = torch.cat([label_all, labels])\n",
    "    \n",
    "    OS_time = label_all[:,1].detach().cpu().numpy()\n",
    "    HR = risk_all.detach().cpu().numpy()\n",
    "    OS = label_all[:,0].detach().cpu().numpy()       \n",
    "    epoch_c_index = concordance_index(OS_time, -HR.reshape(-1), OS)\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    print('Test C-index: {:f}'.format(epoch_c_index))\n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, loss_history, c_index_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b01a21",
   "metadata": {},
   "source": [
    "## model_ADI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2064e842",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepConvSurv()\n",
    "model.apply(init_weights)\n",
    "\n",
    "ADI_Dataloader = split_dataset(ADI_Tile)\n",
    "\n",
    "model_ADI, loss_history_ADI, c_index_history_ADI = train_loop(ADI_Dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "torch.save(model_ADI.state_dict(), './model_ADI_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ef069c",
   "metadata": {},
   "source": [
    "## model_BACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a69c5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepConvSurv()\n",
    "model.apply(init_weights)\n",
    "\n",
    "BACK_Dataloader = split_dataset(BACK_Tile)\n",
    "\n",
    "model_BACK, loss_history_BACK, c_index_history_BACK = train_loop(BACK_Dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "torch.save(model_BACK.state_dict(), './model_BACK_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a57cfe3",
   "metadata": {},
   "source": [
    "## model_DEB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498297ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepConvSurv()\n",
    "model.apply(init_weights)\n",
    "\n",
    "DEB_Dataloader = split_dataset(DEB_Tile)\n",
    "\n",
    "model_DEB, loss_history_DEB, c_index_history_DEB = train_loop(DEB_Dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "torch.save(model_DEB.state_dict(), './model_DEB_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f112dd",
   "metadata": {},
   "source": [
    "## model_LYM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889d6d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepConvSurv()\n",
    "model.apply(init_weights)\n",
    "\n",
    "LYM_Dataloader = split_dataset(LYM_Tile)\n",
    "\n",
    "model_LYM, loss_history_LYM, c_index_history_LYM = train_loop(LYM_Dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "torch.save(model_LYM.state_dict(), './model_LYM_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8561ed63",
   "metadata": {},
   "source": [
    "## model_MUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5d9ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepConvSurv()\n",
    "model.apply(init_weights)\n",
    "\n",
    "MUC_Dataloader = split_dataset(MUC_Tile)\n",
    "\n",
    "model_MUC, loss_history_MUC, c_index_history_MUC = train_loop(MUC_Dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "torch.save(model_MUC.state_dict(), './model_MUC_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331c9a4b",
   "metadata": {},
   "source": [
    "## model_MUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45e07d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepConvSurv()\n",
    "model.apply(init_weights)\n",
    "\n",
    "MUS_Dataloader = split_dataset(MUS_Tile)\n",
    "\n",
    "model_MUS, loss_history_MUS, c_index_history_MUS = train_loop(MUS_Dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "torch.save(model_MUS.state_dict(), './model_MUS_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a561fcd7",
   "metadata": {},
   "source": [
    "## model_NORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c141647c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepConvSurv()\n",
    "model.apply(init_weights)\n",
    "\n",
    "NORM_Dataloader = split_dataset(NORM_Tile)\n",
    "\n",
    "model_NORM, loss_history_NORM, c_index_history_NORM = train_loop(NORM_Dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "torch.save(model_NORM.state_dict(), './model_NORM_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b08731",
   "metadata": {},
   "source": [
    "## model_STR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b31050",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepConvSurv()\n",
    "model.apply(init_weights)\n",
    "\n",
    "STR_Dataloader = split_dataset(STR_Tile)\n",
    "\n",
    "model_STR, loss_history_STR, c_index_history_STR = train_loop(STR_Dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "torch.save(model_STR.state_dict(), './model_STR_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecf0bf3",
   "metadata": {},
   "source": [
    "## model_TUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4b600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepConvSurv()\n",
    "model.apply(init_weights)\n",
    "\n",
    "TUM_Dataloader = split_dataset(TUM_Tile)\n",
    "\n",
    "model_TUM, loss_history_TUM, c_index_history_TUM = train_loop(TUM_Dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "torch.save(model_TUM.state_dict(), './model_TUM_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0a63dd",
   "metadata": {},
   "source": [
    "# Extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318aef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [model_ADI, model_BACK, model_DEB, model_LYM, model_MUC, model_MUS, model_NORM, model_STR, model_TUM]\n",
    "\n",
    "for i in range(len(model_list)):\n",
    "    model_list[i] = model_list[i].to('cuda')\n",
    "    model_list[i].eval()\n",
    "\n",
    "tissue_labels = pd.read_csv('./tissue_labels.csv', index_col = 'tile')\n",
    "labelcsv = pd.read_csv('./survival_COAD_survival.csv', index_col = 'sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee58929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TCGA_COAD_RS_PATH = './randomsampling'\n",
    "foldername = os.listdir(TCGA_COAD_RS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d917130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "Pathology = []\n",
    "TNL = []\n",
    "\n",
    "for folder in tqdm(foldername):\n",
    "    # Pathology\n",
    "    Pathology.append(folder)\n",
    "    \n",
    "    # y\n",
    "    OS = labelcsv.at[folder[0:15], 'OS']\n",
    "    OS_time = labelcsv.at[folder[0:15], 'OS.time']\n",
    "    y.append([OS, OS_time])\n",
    "    \n",
    "    # X\n",
    "    tissue_list = [[], [], [], [], [], [], [], [], []]\n",
    "    for file in os.listdir(os.path.join(TCGA_COAD_RS_PATH, folder)):\n",
    "        t = tissue_labels.at[file, 'tissue']\n",
    "        tissue_list[t].append(file)\n",
    "    \n",
    "    feature_list = []\n",
    "    for i in range(len(tissue_list)):\n",
    "        feature_list.append(torch.zeros((1, 32)).to('cuda'))\n",
    "    \n",
    "    tile_number_list = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    for i in range(len(tissue_list)):\n",
    "        for j in range(len(tissue_list[i])):\n",
    "            img_path = os.path.join(TCGA_COAD_RS_PATH, folder, tissue_list[i][j])\n",
    "            image = Image.open(img_path)\n",
    "            image_tensor = transform(image)\n",
    "            image_tensor.unsqueeze_(0)\n",
    "            image_tensor = image_tensor.to('cuda')\n",
    "            \n",
    "            model_list[i] = model_list[i].to('cuda')\n",
    "            model_list[i].eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                risk = model_list[i](image_tensor)\n",
    "                feature = model_list[i].feature\n",
    "            \n",
    "            feature_list[i] = feature_list[i] + feature\n",
    "            \n",
    "        tile_number_list[i] = len(tissue_list[i])\n",
    "        if len(tissue_list[i]) != 0:\n",
    "            feature_list[i] = feature_list[i] / len(tissue_list[i])\n",
    "        \n",
    "    TNL.append(tile_number_list)\n",
    "     \n",
    "    FEATURE_LIST = []\n",
    "    for i in range(len(feature_list)):\n",
    "        for j in range(len(feature_list[i][0])):\n",
    "            FEATURE_LIST.append(feature_list[i][0][j].item())\n",
    "    \n",
    "    X.append(FEATURE_LIST)\n",
    "\n",
    "#print(f'Pathology : {len(Pathology)}')\n",
    "#print(f'X : {len(X)}*{len(X[0])}')\n",
    "#print(f'y : {len(y)}*{len(y[0])}')\n",
    "#print(f'TNL : {len(TNL)}*{len(TNL[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bdf6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "columns = []\n",
    "\n",
    "for i in range(len(Pathology)):\n",
    "    data.append([Pathology[i]] + y[i] + TNL[i] + X[i])\n",
    "\n",
    "columns.append('pathology')\n",
    "\n",
    "columns.append('OS')\n",
    "columns.append('OS.time')\n",
    "\n",
    "columns.append('ADI.tile')\n",
    "columns.append('BACK.tile')\n",
    "columns.append('DEB.tile')\n",
    "columns.append('LYM.tile')\n",
    "columns.append('MUC.tile')\n",
    "columns.append('MUS.tile')\n",
    "columns.append('NORM.tile')\n",
    "columns.append('STR.tile')\n",
    "columns.append('TUM.tile')\n",
    "\n",
    "for tissue in ['ADI', 'BACK', 'DEB', 'LYM', 'MUC', 'MUS', 'NORM', 'STR', 'TUM']:\n",
    "    for j in range(32):\n",
    "        columns.append(tissue + '.feature' + str(j))\n",
    "        \n",
    "#print(f'{len(data)}*{len(columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad266e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = data, columns = columns)\n",
    "\n",
    "df.to_csv('./hitopathological_features.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
